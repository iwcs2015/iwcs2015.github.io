<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>IWCS 2015</title><link href="http://iwcs2015.github.io/" rel="alternate"></link><link href="http://iwcs2015.github.io/feeds/hackathon.atom.xml" rel="self"></link><id>http://iwcs2015.github.io/</id><updated>2015-02-18T13:32:00+00:00</updated><entry><title>Computational Semantics Hackathon</title><link href="http://iwcs2015.github.io/hackathon.html" rel="alternate"></link><updated>2015-02-18T13:32:00+00:00</updated><author><name>The chairs of IWCS 2015</name></author><id>tag:iwcs2015.github.io,2015-02-18:hackathon.html</id><summary type="html">&lt;p&gt;This year, IWCS will feature a &lt;strong&gt;Computational Semantics Hackathon&lt;/strong&gt;, before the
main conference on &lt;strong&gt;April 11th-12th&lt;/strong&gt; from 10:00 to 17:00.&lt;/p&gt;
&lt;p&gt;We invite researchers, developers, students and users of semantic NLP tools to
participate. The goal of the event is to provide an opportunity to discuss and
develop tools that are used in Computational Semantics. Moreover, we also would
like to attract anyone interested in data processing tools so they could
contribute to open source projects represented at the event. &lt;em&gt;You don't need to
attend the conference to be able to participate at the hackathon.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The event will take place at &lt;a class="reference external" href="http://www.qmsu.org/ground/"&gt;Ground Cafe&lt;/a&gt;. It's
building 33 on the &lt;a class="reference external" href="http://iwcs2015.github.io/static/qm-campus-map.pdf"&gt;QMUL campus map&lt;/a&gt;.
Please refer to the &lt;a class="reference external" href="http://iwcs2015.github.io/location.html"&gt;location page&lt;/a&gt; for
more information on how to get to the QMUL Mile End Campus.&lt;/p&gt;
&lt;p&gt;The hackathon is organised by the Computational Linguistics lab at QMUL and
sponsored by the &lt;a class="reference external" href="http://www.epsrc.ac.uk/"&gt;EPSRC&lt;/a&gt; grant &lt;a class="reference external" href="http://gow.epsrc.ac.uk/NGBOViewGrant.aspx?GrantRef=EP/J002607/1"&gt;EP/J002607/1 â€” Foundational Structures for
Compositional Meaning&lt;/a&gt; and &lt;a class="reference external" href="http://eecs.qmul.ac.uk/"&gt;EECS&lt;/a&gt;. &lt;a class="reference external" href="https://github.com"&gt;GitHub&lt;/a&gt; kindly offers subscription coupons
for the 3 winning teams.&lt;/p&gt;

&lt;div class="section" id="sunday"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id19"&gt;Sunday&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;It's the final day of the hackathon. Coding should stop at about 15:30. Then the
teams will present their projects and we vote to for the best
project/team/presentation.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="saturday"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id20"&gt;Saturday&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We are glad to welcome you to the hackathon! Saturday morning, the time before
the coffee break, is dedicated to introduction. It's also a great opportunity
for you to set up wifi, talk to people and decide on a project.&lt;/p&gt;
&lt;p&gt;We will use a &lt;a class="reference external" href="https://trello.com/b/AQIKkm6V/iwcs-hackathon-2015"&gt;Trello board&lt;/a&gt; to keep track of the progress. You should have received an
invitation email to the board, if not ask someone who is added to it to add you.
Each project is assigned a color, so you can see to which project a card
belongs. &lt;strong&gt;If you are interested in a task, please add yourself to a
corresponding card&lt;/strong&gt;. Feel free to add your own projects in the &lt;a class="reference internal" href="#comments"&gt;Comments&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;From the software point of view, it's nice to have &lt;strong&gt;Python installed&lt;/strong&gt; on your
laptop. We suggest to &lt;a class="reference external" href="http://eecs.io/python-environment-for-scientific-computing.html"&gt;install Miniconda&lt;/a&gt; (there is a USB drive with the
installation scripts, just in case) and create a dedicated virtual environment
with common packages:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Create an environment&lt;/span&gt;
~/miniconda3/bin/conda create -n iwcs15-hack &lt;span class="nv"&gt;python&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;3.4 nltk pandas scikit-learn ipython-notebook

&lt;span class="c"&gt;# Activate it&lt;/span&gt;
&lt;span class="nb"&gt;source&lt;/span&gt; ~/miniconda3/bin/activate iwcs15-hack

&lt;span class="c"&gt;# Install a package using conda (preferred)&lt;/span&gt;
conda install flask

&lt;span class="c"&gt;# Install a package using pip if conda can't find it&lt;/span&gt;
pip install more_itertools
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Register on &lt;a class="reference external" href="https://github.com"&gt;github&lt;/a&gt; and &lt;a class="reference external" href="https://bitbucket.org"&gt;bitbucket&lt;/a&gt;. &lt;a class="reference external" href="https://help.github.com/articles/generating-ssh-keys/"&gt;Generate SSH keys&lt;/a&gt; and the public key to the
services. Use the &lt;a class="reference external" href="https://github.com/iwcs15-hack"&gt;iwcs15-hack organization&lt;/a&gt; to store and share code. Share you
github account name in the &lt;a class="reference internal" href="#comments"&gt;comments&lt;/a&gt;, so we could add you to the organization.&lt;/p&gt;
&lt;p&gt;Get a good text editor, for example &lt;a class="reference external" href="https://atom.io/"&gt;Atom&lt;/a&gt; or &lt;a class="reference external" href="http://www.sublimetext.com/"&gt;SublimeText&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Help others or ask for help! Use &lt;a class="reference internal" href="#comments"&gt;comments&lt;/a&gt; or a &lt;a class="reference external" href="https://tlk.io/iwcs15-hack"&gt;dedicated channel&lt;/a&gt; for
communication. Network. Have fun.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="a-distributional-semantic-toolkit-green"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id21"&gt;A distributional semantic toolkit (Green)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This project aims to provide researchers working in distributional semantics with
a set of core Python utilities. The following functionality is required:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;A space-efficient datastructure for storing distributed representations of words
and phrases, e.g. through memory-mapped &lt;cite&gt;numpy&lt;/cite&gt; arrays or &lt;cite&gt;bcolz&lt;/cite&gt;-backed &lt;cite&gt;pandas&lt;/cite&gt; data frames&lt;/li&gt;
&lt;li&gt;Efficient exact and approximate nearest neighbour search, e.g. through a &lt;cite&gt;scikit-learn&lt;/cite&gt;'s
KD-tree or random projections&lt;/li&gt;
&lt;li&gt;Efficient dimensionality reduction (SVD, NMF) and feature reweighting (PMI, PPMI)&lt;/li&gt;
&lt;li&gt;Converters to and from commonly used formats&lt;/li&gt;
&lt;li&gt;Easy evaluation against a set of word similarity datasets, such as Mitchel and Lapata (2008) or MEN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The project will involve merging and documenting existing pieces of software,
such as &lt;a class="reference external" href="https://github.com/composes-toolkit/dissect"&gt;DISSECT&lt;/a&gt;, &lt;a class="reference external" href="https://github.com/dimazest/fowler.corpora"&gt;fowller.corpora&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/MLCL/DiscoUtils"&gt;discoutils&lt;/a&gt;. Check out &lt;a class="reference external" href="https://github.com/nltk/nltk/issues/798"&gt;a relevant
discussion on including word embedding algorithms to NLTK&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="compositionality-for-distributional-semantic-toolkits-yellow"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id22"&gt;Compositionality for distributional semantic toolkits (Yellow)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;It has been shown that some type-logical grammars can be interpreted in vector
space semantics, so the challenge here would be to build a tool that connects
such a grammar to a distributional setting.&lt;/p&gt;
&lt;p&gt;Ingredients are a representation of such grammars in terms of a
lexicon/derivation rules, a suitable interpretation of types and proofs into
tensor spaces and maps, and distributional data.&lt;/p&gt;
&lt;p&gt;Given a lexicon and derivational rules, a theorem prover such as &lt;a class="reference external" href="http://rise4fun.com/z3"&gt;z3&lt;/a&gt; provides a
proof for a given input sentence which is later used to obtain distributional
representation.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="http://iwcs2015.github.io/static/LGprover2.zip"&gt;LG&lt;/a&gt; is a theorem prover by Jeroen Bransen, see &lt;a class="reference external" href="http://dspace.library.uu.nl/handle/1874/179422"&gt;his MSc thesis&lt;/a&gt;. It's written
in C++, takes a &lt;tt class="docutils literal"&gt;.txt&lt;/tt&gt; file (lexicon) as input and produces a tex/pdf as
output.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="wikipedia-dump-postprocessing-orange"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id23"&gt;Wikipedia dump postprocessing (Orange)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Wikipedia provides &lt;a class="reference external" href="https://dumps.wikimedia.org/enwiki/"&gt;dumps&lt;/a&gt; of all its content. However, to be used by NLP
tools (for example parsers) a dump has to be cleaned up from the wiki markup.
The postrocessing steps are rarely described in details in scientific
literature. A postprocessed Wikipedia dump from 2009 is often used in
current literature.&lt;/p&gt;
&lt;p&gt;The goal of this task is to come up with a easy to deploy and well documented
pipeline of processing a Wikipdedia dump. There are two steps in the pipeline:
raw text extraction and parsing.&lt;/p&gt;
&lt;p&gt;There are at least two ways of getting raw text out of a Wikipedia dump. Wiki
markup can be filtered out using regular expressions, as &lt;a class="reference external" href="https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/wikicorpus.py"&gt;it's done&lt;/a&gt; in
&lt;a class="reference external" href="https://radimrehurek.com/gensim/"&gt;gensim&lt;/a&gt; and &lt;a class="reference external" href="https://github.com/bwbaugh/wikipedia-extractor"&gt;Wikipedia Extractor&lt;/a&gt;. Alternatively, text in the wiki markup can
be parsed using &lt;a class="reference external" href="https://www.mediawiki.org/wiki/Parsoid"&gt;Parsoid&lt;/a&gt; to obtain (X)HTML, later this HTML is processed, for
example tables and images are removed (see &lt;a class="reference external" href="http://nbviewer.ipython.org/urls/bitbucket.org/dimazest/phd-buildout/raw/tip/notebooks/Wikipedia%20dump.ipynb"&gt;this notebook&lt;/a&gt;). &lt;a class="reference external" href="http://johnmacfarlane.net/pandoc/"&gt;Pandoc&lt;/a&gt; and
&lt;a class="reference external" href="https://github.com/docverter/docverter#docverter-server"&gt;Docverter&lt;/a&gt; is a powerful document conversion solution that can be used to
convert a wiki dump to plain text.&lt;/p&gt;
&lt;p&gt;Later the raw text of a dump can be parsed by some of these parsers:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://svn.ask.it.usyd.edu.au/trac/candc"&gt;C&amp;amp;C tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://cogcomp.cs.illinois.edu/page/software"&gt;Illinois tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.maltparser.org/"&gt;MaltParser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://ml.nec-labs.com/senna/"&gt;Senna&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://nlp.stanford.edu/software/corenlp.shtml"&gt;Stanford CoreNLP&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.ark.cs.cmu.edu/TurboParser/"&gt;TurboParser&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://github.com/yahoo/YaraParser"&gt;YaraParser&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It might be worth submitting the results to &lt;a class="reference external" href="https://www.sigwac.org.uk/wiki/WAC10"&gt;10th Web as Corpus Workshop
(WaC-10)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There is work in progress on making HTML dumps available, see &lt;a class="reference external" href="https://phabricator.wikimedia.org/T93396"&gt;T93396&lt;/a&gt; and
&lt;a class="reference external" href="https://phabricator.wikimedia.org/T17017"&gt;T17017&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="nltk-corpus-readers-red"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id24"&gt;NLTK corpus readers (Red)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;a class="reference external" href="http://www.nltk.org/"&gt;NLTK&lt;/a&gt; is a natural language toolkit that provides basic
tools to deal with textual information. &lt;a class="reference external" href="http://www.nltk.org/api/nltk.corpus.reader.html#module-nltk.corpus.reader"&gt;Corpus readers&lt;/a&gt; are interfaces to
access textual resources (called corpora). The task is to provide interfaces to
the following resources.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;strong&gt;Groningen Meaning Bank&lt;/strong&gt;: the &lt;a class="reference external" href="http://gmb.let.rug.nl/"&gt;Groningen Meaning Bank&lt;/a&gt; is a free
semantically annotated corpus that anyone can edit.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;UkWaC&lt;/strong&gt;: &lt;a class="reference external" href="http://wacky.sslmit.unibo.it/doku.php"&gt;UkWaC&lt;/a&gt; is a 2 billion
word corpus constructed from the Web   limiting the crawl to the .uk domain.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;AMR&lt;/strong&gt;: the &lt;a class="reference external" href="http://amr.isi.edu/index.html"&gt;AMR Bank&lt;/a&gt; is a set of English sentences paired with simple,
readable semantic representations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="tweet-paraphrase-generator-violet"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id25"&gt;Tweet paraphrase generator (Violet)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Given a tweet, the system has to come up with a paraphrase. For example, by
substituting all the content words (nouns, verbs, adjectives and adverbs) with
similar words.&lt;/p&gt;
&lt;p&gt;A twitter bot should monitor Twitter for tweets that contain &lt;a class="reference external" href="https://twitter.com/search?q=%23iwcs%20OR%20%23iwcs2015"&gt;#iwcs or #iwcs2015&lt;/a&gt; and generate a paraphrase tweet. Also,
tweets directed to the bot should be replied with a paraphrase.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="twitter-stream-analysis-blue"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id26"&gt;Twitter stream analysis (Blue)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;We are collecting tweets about Easter, Cricket World Cup, IWCS, UKG Fest,
London, and the London Marathon. In addition we are gathering geo located tweets
from the UK. The task is to give insights of what these streams are about. Some
limited statistics about the collected tweets:&lt;/p&gt;
&lt;pre class="literal-block"&gt;
du -hs *
632M  cricket
816M  easter
13M ep14
199M  heartbleed
56K iwcs
8.1G  london
2.1M  london-marathon
2.0G  uk
1.9M  ukg-fest
&lt;/pre&gt;
&lt;p&gt;&lt;a class="reference external" href="http://poultry.readthedocs.org/en/latest/"&gt;Poultry&lt;/a&gt; is a tweet collection manager that might be handy that provides a
&lt;a class="reference external" href="http://poultry.readthedocs.org/en/latest/#integration-with-other-tools"&gt;simple access to a tweet collection&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="dialog-system-light-blue"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id27"&gt;Dialog system (Light Blue)&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Matthew Stone provided a series of IPython Notebooks (&lt;a class="reference external" href="https://github.com/iwcs15-hack/dialog_system"&gt;github repo&lt;/a&gt;, &lt;a class="reference external" href="http://nbviewer.ipython.org/github/iwcs15-hack/dialog_system/tree/master/"&gt;rendered
notebooks&lt;/a&gt;) that implement and extend the original Eliza program, and build a
dialog move classifier using NLTK and use information retrieval to put together
relevant responses.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="other-resources"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id28"&gt;Other resources&lt;/a&gt;&lt;/h2&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.eecs.qmul.ac.uk/~dm303/cvsc14.html#experiment-data"&gt;Distributional vectors&lt;/a&gt; for 23586 words extracted from Google Books Ngrams.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing"&gt;GoogleNews-vectors-negative300.bin.gz&lt;/a&gt; &lt;tt class="docutils literal"&gt;word2vec&lt;/tt&gt; vectors use
&lt;a class="reference external" href="http://radimrehurek.com/gensim/models/word2vec.html"&gt;gensim.models.word2vec&lt;/a&gt; to access the word vectors and perform similarity
queries.&lt;/li&gt;
&lt;li&gt;&lt;a class="reference external" href="http://www.cl.cam.ac.uk/~fh295/simlex.html"&gt;SimLex999&lt;/a&gt; is a gold standard resource for the evaluation of models that learn
the meaning of words and concepts.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class="section" id="more-project-ideas"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id29"&gt;More project ideas&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Participants and sponsors are welcome to propose any and all ideas relating to
computational semantics - please &lt;a class="reference external" href="mailto:d.milajevs@qmul.ac.uk?subject=IWCS-Hackathon"&gt;get in touch&lt;/a&gt;, submit a pull request with
your idea added to &lt;a class="reference external" href="https://github.com/iwcs2015/iwcs2015.github.io/blob/pelican/content/articles/07-hackathon.rst"&gt;this page&lt;/a&gt;, or just write it down in the &lt;a class="reference internal" href="#comments"&gt;comments&lt;/a&gt; below&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="contact-information"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id30"&gt;Contact information&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;In case you are interested in supporting the event contact Dmitrijs Milajevs
&amp;lt;&lt;a class="reference external" href="mailto:d.milajevs@qmul.ac.uk"&gt;d.milajevs@qmul.ac.uk&lt;/a&gt;&amp;gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="comments"&gt;
&lt;h2&gt;&lt;a class="toc-backref" href="#id31"&gt;Comments&lt;/a&gt;&lt;/h2&gt;
&lt;div id="disqus_thread"&gt;&lt;/div&gt; &lt;script type="text/javascript"&gt;     /* * * CONFIGURATION VARIABLES * * */     var disqus_shortname = 'iwcs2015';      /* * * DON'T EDIT BELOW THIS LINE * * */     (function() {         var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;         dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';         (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);     })(); &lt;/script&gt; &lt;noscript&gt;Please enable JavaScript to view the &lt;a href="https://disqus.com/?ref_noscript" rel="nofollow"&gt;comments powered by Disqus.&lt;/a&gt;&lt;/noscript&gt;&lt;/div&gt;
</summary></entry></feed>