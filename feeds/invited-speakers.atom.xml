<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>IWCS 2015</title><link href="http://iwcs2015.github.io/" rel="alternate"></link><link href="http://iwcs2015.github.io/feeds/invited-speakers.atom.xml" rel="self"></link><id>http://iwcs2015.github.io/</id><updated>2015-02-20T12:16:00+00:00</updated><entry><title>Invited Speakers</title><link href="http://iwcs2015.github.io/speakers.html" rel="alternate"></link><updated>2015-02-20T12:16:00+00:00</updated><author><name>The chairs of IWCS 2015</name></author><id>tag:iwcs2015.github.io,2015-02-20:speakers.html</id><summary type="html">
&lt;div class="section" id="id1"&gt;
&lt;h2&gt;&lt;a class="reference external" href="http://www.cl.cam.ac.uk/~aac10/"&gt;Ann Copestake&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;cite&gt;University of Cambridge&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;TBA&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id2"&gt;
&lt;h2&gt;&lt;a class="reference external" href="http://www.iro.umontreal.ca/~bengioy/yoshua_en/index.html"&gt;Yoshua Bengio&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;cite&gt;Université de Montréal&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Deep Learning of Semantic Representations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The core ingredient of deep learning is the notion of distributed
representation. This talk will start by explaining its theoretical advantages,
in comparison with non-parametric methods based on counting frequencies of
occurrence of observed tuples of values (like with n-grams). The talk will then
explain how having multiple levels of representation, i.e., depth, can in
principle give another exponential advantage. Neural language models have been
extremely successful in recent years but extending their reach from language
modelling to machine translation is very appealing because it forces the learned
intermediate representations to capture meaning, and we found that the resulting
word embeddings are qualitatively different. Recently, we introduced the notion
of attention-based neural machine translation, with impressive results on
several language pairs, and these results will conclude the talk.&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="id3"&gt;
&lt;h2&gt;&lt;a class="reference external" href="http://people.csail.mit.edu/regina/"&gt;Regina Barzilay&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;cite&gt;Massachusetts Institute of Technology&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Semantics of Language Grounding&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this talk, I will address the problem of natural language grounding. We
assume access to natural language documents that specify the desired behavior of
a control application. Our goal is to generate a program that will perform the
task based on this description. The programs involve everything from changing
the privacy settings on your browser, playing computer games, performing complex
text processing tasks, to even solving math problems. Learning to perform tasks
like these is complicated because the space of possible programs is very large,
and the connections between the natural language and the resulting programs can
be complex and ambiguous.  I will present methods that utilize semantics of the
target domain to reduce natural language ambiguity.  On the most basic level,
executing the induced programs in the corresponding environment and observing
their effects can be used to verify the validity of the mapping from language to
programs.  We leverage this validation process as the main source of supervision
to guide learning in settings where standard supervised techniques are not
applicable. Beyond validation feedback, we demonstrate that using semantic
inference in the target domain (e.g., program equivalence) can further improve
the accuracy of natural language understanding.&lt;/p&gt;
&lt;/div&gt;
</summary></entry></feed>